# MAP1 Study

## 1. What is the five number summary of a distribution and what is it useful for?

Five number summary: **min** , **$Q_1$** , **median**, **$Q_3$**, **max**.

- **min** is the lowest value
- $Q_1$: 25% of the data falls bellow this value
- $Q_3$: 75% of the data falls bellow this value 
- **median**: middle value if odd number of values, or average of the two mid values otherwise
- **max** is the greatest value

## 2. Why would you want to use a histogram when analyzing data? 

It shows what proportion of cases fall into each of several categories. These categories are usually specified as non-overlapping intervals of some variable. Raw data is data that has not been processed for use, so in order to analyze it, is convenient to see how it distributes into the different categories/features that are envolved.

## 3. Why should you do data cleaning before applying any intelligent technique? 

Data in the real world has a lot of potentially incorrect data due to faulty instruments, human or computer error, transmission error, etc. Data can be incomplete, noisy, inconsistent and intentional. In order to prepare the data to apply any intelligent technique we must clean it so we can have good statistical results and not produce biased estimates (leading to invalid conclusions). 

Data cleaning means fill in the missing values, smooth noisy data, identify or remove outliers, and resolve inconsistencies.

## 4. When dealing with missing data, what are the advantages/disadvantages of filling any missing values with the mean value of the respective attribute? 

Advantages:
- Simple
- Potential to reduce bias by using all study data to estimate response for missing subjects.

Disadvantages:
- Significantly decreased standard deviation (variance)
- Increased type 1 error
- Overestimation 

If the data is skewed, the median value is a batter choice.

## 5. Why would you use Z-Score instead of Min-Max when normalizing data?

Normalization is scale the values to fall withing a smaller, specified range. 
There is min-max and z-score normalization. This is important because many algorithms attempt to find trends in the data by comparing features of data points, but issus might arise when the features are on different scales (so normalization helps avoiding this).

Despite z-score standardization does not preserve the relationship among the original data (it changes the data quite a bit) unlike min-max (because it is a linear transformation of the original data), z-score standardization handles outliers better and it is good for when the data range is unknown. 

## 6. Consider the following dataset that indicates the distribution of grades of a give course

`[Grade, #Students] = [[0,0];[1,0];[2, 4];[3,0];[4,7];[5,8];[6,6];[7,6];[8,6];[9,2];[10,1]]`

### a) Does the data represent a histogram? 

Yes. In an histogram x-axis are values and y-axis are frequencies. In this case "Grade" is the x-axis values and "#Students" the y-axis frequencies.

### b) Is the dataset symmetric, positively skewed or negatively skewed? 

Symmetric: mode = median = mean
Positively skewed: mode < median < mean
Negatively skewed: mode > median > mean

**mode**: value that occurs most frequently in the data. In this case its 5. 

**mean** (algebraic measure): 

$$
mean=\frac{1}{40}*(0*0+1*0+2*4+3*0+4*7+5*8+6*6+7*6+8*6+9*2+10*1)=5,75
$$

median: middle value if odd number of values or average of the middle two value otherwise. 
median = 5

**Conclusion**: Since mode < median < mean this dataset is positively skewed. 

## 7. What is an outlier? 

An outliers is a data object that deviates significantly from the normal objects as if it were generated by a different mechanism. Usually it is not noise or error, but a higher/lower values than 1.5*IQR (Inter Quartile Range ? $Q_3$-$Q_1$)

## 8. What is cross-validation and when should you use it? 

Cross-validation is a standard tool in statistics that provides an appealing guiding principle choosing (within a set of parameters) the "best" model according to a certain criterion. Cross validation is an alternative to just simple separate the data for training, validation and test. 

First, that available data set is randomly partitioned into a training sample and a test set. The training sample is further partitioned into two disjoint subsets: 
- An estimation set, used to select the model; 
- A validation subset, used to test of validate the model. 

The motivation is to validate the model on a data set different from the one used for parameter estimation. Each sample is used the same number of time for training and once for testing. In this way, we may use the training sample to evaluate the performance of various candidate models and then choose the "best" one.
Cross validation is appealing particularly when we have to design a large neural network with good generalization as the goal in different ways.

## 9. Given the following confusion matrix, calculate the precision, recall and F1:    

![picture 1](%24%7Bpath%7D/img/b642f3e1c59b1ffee65436c74b64961d870c2d4eb64f365871861d174e980d46.png)  

$$
\mathrm{Accuracy}=\frac{TP+TN}{P+N}
$$

How many predictions are correct over all the cases?

$$
\mathrm{Precision}=\frac{TP}{TP+FP}
$$

Positive predicted value (PPV)

$$
\mathrm{Recall/Sensitivity}=\frac{TP}{P}
$$

True Positive Rate (TPR)

$$
\mathrm{F1}=\frac{2TP}{2TP+FP+FN}
$$

Harmonic mean of PPV an TPR

$$
\mathrm{Specificity}=\frac{TN}{N}
$$

True Negative Rate (TNR) -> Opposite of recall

$$
\mathrm{Precision}=\frac{122}{122+56}=0,652
$$

$$
\mathrm{Recall}=\frac{122}{122+26}=0,824
$$

$$
\mathrm{F1}=\frac{2*122}{2*122+65+26}=0,728
$$

## 10. What should and shouldn't do if your data is highly unbalanced?

In a balanced dataset, all classes are equally represented. 
In an unbalanced dataset some classes are more prevalent than others.
Balancing the training set is an option, but the validation and test sets should always heep the imbalance of the original dataset, otherwise the results are not reliable. 

We should use the right evaluation metrics, because some metrics are sensitive to the data balance and an evaluation that only uses such metrics hides the performance of the algorithm. All metrics that are calculated based of a single column of the confusion matrix are insensitive to the data balance (recall, specificity)

## 11. Why cant a perceptron learn a XOR function? 

The perceptron can't solve any problem that is not linearly separable. 
It can only implement functions where all points on one side of the hyperplane belong to the same class. 
So there is no combination of weights or T that allows the perceptron to implement a function that outputs 1 if two inputs are different, and 0 otherwise.

## 12. Can a multilayer perceptron with a single hidden layer lean any continuous function? 

With 1 number of hidden layers: "Can approximate any function that contains a continuous mapping from one infinite space to another."

A single perceptron can't deal with non-linear problems. However, by organizing a given number of perceptron into at least 3 layers (1 hidden), one obtains a trainable neural network that can behave as an universal approximator.

## 13. What are hidden nodes? 

A node, also called a neuron or perceptron, is a computational unit that has one of more weighted input connections, a transfer function that combined the inputs in some way, and an output connection. Nodes are then organized into layers to comprise a network. 
The hidden nodes are the nodes in the hidden layers (neither the first of the last layer. They receiver weighted inputs from the previous layer and calculate a nonlinear mapping using the activation function. 

## 14. Why should you avoid using a hight number of neurons in a hidden layer of a MLP?

Theoretically, a single hidden layer is enough to solve any problem, but making a single hidden layer network learn a complex problem might be tricky and hight dependent of the quality of the training set. So, using additional layers is advised for more complex problems.

- Too few neurons -> underfitting (not enough neurons to capture the problem intricacies)
- Too much neurons -> overfitting (the information in the training set is not enough to train all the neurons in the hidden layers)


## 15. Consider the following NN: 


![picture 2](%24%7Bpath%7D/img/9bd80126fcac0924f4a3d5d817de0d197b989a143baf441dfe411001d39d4393.png)  

What is the usual sequence of events for training the
network using the backpropagation algorithm?

d. (1) calculate yj = f(Hj), (2) calculate zk = f(Ik), (3) update wkj, (4) update vji.

## 16. How many different patterns can be learned by a Hopfield Network with 500 hidden nodes?

Capacity: 138 patterns can be recalled from storage (learnt) for every 1000 nodes.

So, in the case of 500 nodes it will be $\frac{138}{2}=69$ patterns.

## 17. What is known as a "regularization" in a NN learning process? What is it used for?

Techniques that modifies the learning algorithm to generalize better (improve model performance on the unseen data). Examples: Weight (L1 & L2) Regularization, Dropout and Early Stopping.

## 18. 4rf5vtWich of the follwing operations are valid T-norms in Fuzzy Sets

**T-norm** is a function that is commutative, associative and monotone

R: Minimum and Product 

## 19. Suppose X={-3, -2, -1, 0, 1, 2, 3}. Let fuzzy subsets A and B of X be defined by their membership vectors A = (0.0, 0.3, 0.8, 1.0, 0.8, 0.3, 0.0) and B = (1.0, 0.9, 0.7, 0.5, 0.2, 0.0, 0.0). Compute A∪B and A∩B using the original Zadeh definitions.

A U B -> Vês o maximo entre os dois e é esse: 
A $\mathrm{U}^{-1}$ B -> Minimum value between sets
AUB=(1.0,0.9,0.8,1.0,0.8,1.0,0.8,0.3,0.0)


## 20. Consider the following fuzzy set A: 

![picture 1](%24%7Bpath%7D/img/4ced3c159d15b0f0557e349e2c4c58e3e028ef102bac973ae6fd2453c94d2fcd.png)  

### Compute $A_{0.7}$

$x+3=0.7$ and $-x-1=0.7$

Solve this and the result is : 

$A_{0.7}\in ]-2.7,-1.7[$

## 21. In a fuzzy logic control application, explain what is meant by the terms “fuzzification” and “defuzzification.” Why are they necessary?

Fuzzification is the process of transforming a crisp set to a fuzzy set and defuzzification is the process of reduzing a fuzzy set into a crisp set.

It is necessary to have two interfaces between the outside world and the fuzzy system. The fuzzifier that determinates the match between a given input and the linguistic calculate the membership degrees of the measured inputs to the linguistic terms in the fuzzier. The defuzzier is the interface between the fuzzy systems and the outside world (output side), it is needed when a crisp number is required (in the real world). So, it computes a crisp number that represents the output fuzzy set.

## 22. Consider a fuzzy variable "health". Define a domain and the linguistic terms, so that some (hopefully) measured index could be used to activate a rule with Health in one of its antecedent clauses. 

![picture 1](%24%7Bpath%7D/img/0b33fc67979a34d14fdf6dbc400e2b12202a2b84ddf1c4d1883619de9bd11cd8.png)  

